import numpy as np
import time
import os
import gc

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import scipy
from scipy import signal

from tqdm.auto import tqdm
import h5py
import glob
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
import sys
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore", message="Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created")
warnings.filterwarnings("ignore", message="nn.functional.tanh is deprecated. Use torch.tanh instead.")
warnings.filterwarnings("ignore", message="nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
warnings.filterwarnings("ignore", category=DeprecationWarning)

sys.path.append("/projects/bbvf/victoria/WaveNet_training/2-channel")

from models_torch import *
from data_generators_torch import *

############################# plot dataset #####################
def plot_waveforms(wf_dataset, noise_ranges):
    for noise_range in noise_ranges:
        wf_dataset.noise_range = noise_range

        plt.figure(figsize=(20, 8))
        for i in range(3):
            X, y = wf_dataset.__getitem__(i)  
            labels=y[:,0]
            y = np.arange(4096)

            plt.subplot(3, 5, i + 1)
            plt.plot(y, X[:, 0].numpy(), label='L1',linewidth=0.7)
            plt.plot(y, X[:, 1].numpy(), label='H1',linewidth=0.7)
            plt.plot(y, X[:, 2].numpy(), label='V1',linewidth=0.7)
            plt.plot(y, labels, label='label',linewidth=0.7,c='black')
            plt.title(f'Noise Range {noise_range}')
            plt.legend(loc='upper right')
        plt.tight_layout()
        plt.show()
        
def plot_whitened_waveforms(wf_dataset, noise_ranges):
    for noise_range in noise_ranges:
        wf_dataset.noise_range = noise_range

        plt.figure(figsize=(20, 8))
        for i in range(1):
            # Generate original signal
            X, y = wf_dataset.__getitem__(i)  
            labels=y[:,0]
            y = np.arange(4096)

            # Generate whitened signal
            strain_L1 = X[:, 0].numpy()
            strain_H1 = X[:, 1].numpy()
            strain_V1 = X[:, 2].numpy()

            psd_L1 = pickle.load(open(wf_dataset.psd_L1_files, 'rb'), encoding="bytes")
            psd_H1 = pickle.load(open(wf_dataset.psd_H1_files, 'rb'), encoding="bytes")
            psd_V1 = pickle.load(open(wf_dataset.psd_V1_files, 'rb'), encoding="bytes")

            strain_whiten_L, strain_whiten_H, strain_whiten_V = whiten.whiten_signal(
                strain_L1, strain_H1, strain_V1, wf_dataset.dt, psd_L1, psd_H1, psd_V1
            )

            # Plot original signal
            plt.subplot(3, 4, 4 * i + 1)
            plt.plot(y, strain_L1, label='L1',linewidth=0.7)
            plt.plot(y, strain_H1, label='H1',linewidth=0.7)
            plt.plot(y, strain_V1, label='V1',linewidth=0.7)
            plt.plot(y, labels, label='label',linewidth=0.7,c='black')
            plt.title(f'Original Signal')
            plt.legend(loc='upper right')

            # Plot whitened signal
            plt.subplot(3, 4, 4 * i + 2)
            plt.plot(y, strain_whiten_L, label='L1',linewidth=0.7)
            plt.plot(y, strain_whiten_H, label='H1',linewidth=0.7)
            plt.plot(y, strain_whiten_V, label='V1',linewidth=0.7)
            plt.plot(y, labels, label='label',linewidth=0.7,c='black')
            plt.title(f'Whitened Signal')
            plt.legend(loc='upper right')

        plt.tight_layout()
        plt.show()

wf_dataset = WFGDataset(
    noise_dir='/projects/bbvf/victoria/WaveNet_data/Gaussian_Noise/',
    data_dir='/projects/bbvf/victoria/WaveNet_data/combined_spin/',
    batch_size=32,
    dim=4096,
    n_channels=3,
    shuffle=False,
    train=0,
    gaussian=1,  
    noise_prob=0,
    noise_range=[0.1, 0.3],  
    initial_epoch=1
)

########################### inference functions #####################

class TimeSeriesDataset(Dataset):
    def __init__(self, data, targets, length, stride=1, start_index=0, end_index=None):
        self.data = data
        self.targets = targets
        self.length = length
        self.stride = stride

        if end_index is None or end_index > len(data):
            end_index = len(data) - 1

        self.start_index = start_index
        self.end_index = end_index

        self.sample_indices = np.arange(self.start_index, self.end_index - self.length + 1, self.stride)

    def __len__(self):
        return len(self.sample_indices)

    def __getitem__(self, idx):
        start_idx = self.sample_indices[idx]
        end_idx = start_idx + self.length

        # Slice the data and targets for the current sample
        sample_data = self.data[start_idx:end_idx]
        sample_target = self.targets[start_idx:end_idx]
        return sample_data.float(), sample_target.float()

def normalize(strain):
    std = np.std(strain[:])
    mean=np.mean(strain[:])
    strain[:]+=-mean
    strain[:] /= std
    return strain

def butter_bandpass_filter(strain, fs=4096, lowcut=10, highcut=1000, order=4, buffer=2048):
    padded_strain = strain#np.pad(strain, (buffer, buffer), mode='constant')
    
    nyq = 0.5 * fs
    b, a = scipy.signal.butter(order, [lowcut / nyq, highcut / nyq], btype='band')
    filtered = scipy.signal.filtfilt(b, a, padded_strain)
    filtered = filtered[buffer:-buffer]
    
    return filtered


# Function to find peaks
def find_peaks(preds, threshold=0.9, width=1000, mean=0.9):
    test_p = preds
    peaks, properties = signal.find_peaks(test_p, height=threshold, width=width, distance=4096 * 1)
    left = properties['left_ips']
    right = properties['right_ips']

    f_left = []
    f_right = []
    for i in range(len(left)):
        sliced = test_p[int(left[i]):int(right[i])]
        if (np.mean(sliced > mean) > 0.5):
            f_left.append(int(left[i]))
            f_right.append(int(right[i]))

    return peaks, f_left, f_right


# Function to merge windows
def merge_windows(triggers_0, triggers_5):
    triggers = {}
    for key in triggers_0.keys():
        right_0 = triggers_0[key]
        right_5 = triggers_5[key]

        combined = right_0.copy()
        for r_5 in right_5:
            keep = True
            for r_0 in right_0:
                if abs(r_5 - r_0) == 1 / 2:
                    keep = False
            if keep:
                combined.append(r_5)

        triggers[key] = combined

    return triggers

def make_preds(whitened_L1, whitened_H1, whitened_V1, model, inference_args,
               dataset_name):
    device = next(model.parameters()).device

    offsets = [0,
               2047 // 2,                # 1 023   ≈ 0.25 s
               2047,                     # 2 047   ≈ 0.50 s  
               2047 + 2047 // 2]         # 3 070   ≈ 0.75 s

    # run a single dataloader through the network
    def _single_pred(dataloader, bar_pos, disable_bar):
        preds = []
        pbar = tqdm(dataloader, leave=True, position=bar_pos,
                    desc=f'Predicting {dataset_name}[{bar_pos}]',
                    dynamic_ncols=True, disable=disable_bar)
        for batch in pbar:
            with torch.no_grad():
                batch = batch[0].to(device)
                preds.append(model(batch).cpu().numpy())
        return np.concatenate(preds).ravel() if preds else np.empty(0)

    # ── normalise & tensor-ise input ───────────────────────────────────────────
    n = min(len(whitened_L1), len(whitened_H1), len(whitened_V1))
    whitened_L1, whitened_H1, whitened_V1 = (
        torch.tensor(normalize(x[:n]).copy(), dtype=torch.float32)
        for x in (whitened_L1, whitened_H1, whitened_V1)
    )
    data = torch.stack((whitened_L1, whitened_H1, whitened_V1), dim=1).to(device)

    # ── build dataloaders for all offsets ──────────────────────────────────────
    loaders = [
        DataLoader(
            TimeSeriesDataset(data, data, length=4096, stride=4096,
                              start_index=off),
            batch_size=inference_args.batch_size, shuffle=False)
        for off in offsets
    ]

    # ── run the model in parallel ──────────────────────────────────────────────
    disable_bars = [False] + [True] * (len(loaders) - 1)
    with ThreadPoolExecutor(max_workers=len(loaders)) as pool:
        futures = [
            pool.submit(_single_pred, dl, i, dis)
            for i, (dl, dis) in enumerate(zip(loaders, disable_bars))
        ]
    preds = [f.result() for f in futures]   

    gc.collect()
    torch.cuda.empty_cache()
    return preds, offsets


def get_triggers(preds_list, offsets, width, threshold,
                 truncation=0, fs=4096):
    """
    Returns {'detection': [t₁, t₂, …]} where each tᵢ is in *seconds*.
    Duplicate triggers arising from overlapping windows are merged
    if they lie closer than 0.25 s (one quarter-window) to a previous one.
    """
    assert len(preds_list) == len(offsets)

    all_triggers = []
    dynamic_mean = max(0, min(0.95, threshold - 0.05))

    for preds, off in zip(preds_list, offsets):
        _, _, right = find_peaks(preds,
                                 threshold=threshold,
                                 width=width,
                                 mean=dynamic_mean)
        # convert sample indices to seconds and apply offset
        all_triggers.extend([(r + truncation + off) / fs for r in right])

    # ── remove duplicates from different windows within 1/4 s  ─────────────
    all_triggers = sorted(all_triggers)
    merged = []
    for t in all_triggers:
        if not merged or t - merged[-1] > 0.25:
            merged.append(t)

    return {'detection': merged}


def process_data(data_dir, model, threshold, width, inference_args):
    dataset_name = os.path.splitext(os.path.basename(data_dir))[0]

    with h5py.File(data_dir, 'r') as fp:
        strain_L1 = fp['strain_L1'][:]
        strain_H1 = fp['strain_H1'][:]
        strain_V1 = fp['strain_L1'][:]              

    n = min(len(strain_L1), len(strain_H1), len(strain_V1))
    strain_L1, strain_H1, strain_V1 = (x[:n] for x in (strain_L1,
                                                       strain_H1,
                                                       strain_V1))

    # ── inference ─────────────────────────────────────────────────────────────
    t0 = time.time()
    preds_list, offsets = make_preds(strain_L1, strain_H1, strain_V1,
                                     model, inference_args, dataset_name)
    print(f'Inference   ↯ {time.time() - t0:.1f}s')

    # ── peak finding / trigger extraction ─────────────────────────────────────
    t0 = time.time()
    triggers = get_triggers(preds_list, offsets, width, threshold,
                            truncation=0)
    print(f'Postprocess ↯ {time.time() - t0:.1f}s')

    gc.collect()
    torch.cuda.empty_cache()
    return dataset_name, triggers, strain_L1, strain_H1, strain_V1

    
######################## process and plot triggers ################################################

def tolerant_intersection(*trigger_lists, tolerance=1):
    common_triggers = set(trigger_lists[0])
    for triggers in trigger_lists[1:]:
        common_triggers = {trigger for trigger in common_triggers if any(np.isclose(trigger, t, atol=tolerance) for t in triggers)}
    return list(common_triggers)

def process_triggers(dataset_name_to_dir, dataset_triggers, models, tolerance=0.05, plot=True):
    common_triggers_count = 0
    common_triggers_info = {}
    
    for dataset_name, triggers_dicts in dataset_triggers.items():
        if len(triggers_dicts) == len(models):
            trigger_lists = [triggers_dict['detection'] for triggers_dict in triggers_dicts]
            common_triggers = tolerant_intersection(*trigger_lists, tolerance=tolerance)
            
            for trigger in common_triggers:
                if 1923 <= trigger <= 1927:
                    common_triggers_count += 1
            
            common_triggers_info[dataset_name] = common_triggers
            
            if plot:
                data_dir = dataset_name_to_dir[dataset_name]
                with h5py.File(data_dir, 'r') as fp:
                    strain_L1 = fp['strain_L1'][:]
                    strain_H1 = fp['strain_H1'][:]
                    strain_V1 = fp['strain_L1'][:]
                    
                    min_length = min(len(strain_L1), len(strain_H1))
                    strain_L1 = strain_L1[:min_length]
                    strain_H1 = strain_H1[:min_length]
                    strain_V1 = strain_V1[:min_length]
                    
                plot_results(dataset_name, strain_L1, strain_H1, strain_V1, common_triggers, save_path=f'{dataset_name}.png')
    
    return common_triggers_info, common_triggers_count


            
def plot_results(dataset_name, strain_L1, strain_H1, strain_V1, common_triggers, save_path=None, demo=0):
    strain_L1=normalize(strain_L1)
    strain_H1=normalize(strain_H1)
    strain_V1=normalize(strain_V1)
    plt.figure()
    fig, axs = plt.subplots(3, 1, figsize=(10, 14))
    
    x_vals = np.arange(len(strain_L1)) / 4096
    merger_time=np.size(strain_L1)//2//4096
    print('ground truth merger time:',merger_time)
    

    
    axs[0].plot(x_vals[::4], strain_L1[::4], label='Livingston (L1)')
    axs[0].set_title('Livingston (L1)')
    axs[0].set_ylim([-7, 7])
    axs[0].legend(loc='upper right')

    axs[1].plot(x_vals[::4], strain_H1[::4], label='Hanford (H1)')
    axs[1].set_title('Hanford (H1)')
    axs[1].set_ylim([-7, 7])
    axs[1].legend(loc='upper right')


    axs[2].plot(x_vals[::4], np.zeros_like(x_vals[::4]))
    axs[2].axvline(x=merger_time, color='r', linestyle='-', ymin=0.25, ymax=0.75, label='True Signal', linewidth=4)
    for trigger in common_triggers:
        axs[2].axvline(x=trigger, color='g', linestyle='--', ymin=0.0, ymax=1.0, label='Predicted Signal' if trigger == common_triggers[0] else "")
    axs[2].set_title('Predicted Signals')
    axs[2].legend(loc='upper right')

    if demo == 1:
        fig.suptitle(f'Dataset: {dataset_name}, (no ensemble averaging)', fontsize=16)
    else:
        fig.suptitle(f'Dataset: {dataset_name}', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.93])
    fig.subplots_adjust(top=0.90)

    if save_path:
        plt.savefig(save_path)
    plt.show()
    plt.close(fig)

    